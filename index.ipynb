{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl2jrNQzg5Tt"
      },
      "source": [
        "# Tuning Neural Networks with Normalization, Evaluation, and Regularization - Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3pwVxigg5Tv"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "Now that you have a general sense of the architecture of neural networks and some of their underlying concepts, its time to further investigate how to properly tune a model for optimal performance.\n",
        "\n",
        "Before we do that, we'll consider what is meant by *deep learning*, which are neural networks with multiple layers. In other words, these neural networks have a lot of depth; hence, *deep learning*."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2xwg27g5Tv"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We're familiar with evaluation methods such as looking at metrics and using K-Folds. While we have naturally already used metrics with neural networks, we'll now see how to use validation methods like K-Folds. Additionally, we'll be introduced to `TensorBoard`, which is essentially a dashboard for `TensorFlow`. It will help us evaluate our model.\n",
        "\n",
        "## Normalization\n",
        "\n",
        "Another modeling problem occurs when one gets trapped into a local minimum when searching for an optimal solution using an iterative approach such as gradient descent. One technique for counteracting this scenario is normalizing features. Normalization in deep learning models can drastically decrease computation time, mitigate common issues such as vanishing or exploding gradients, and increase model performance.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "You've seen regularization before in many other models including linear regression. For example, recall the L1 and L2 penalties which modify ordinary linear regression. These updated loss functions can help tune models so they do not overfit to the training data. For neural networks, you'll use a surprisingly similar process in order to achieve well trained models that are neither overfit nor underfit.\n",
        "\n",
        "\n",
        "### Optimization\n",
        "\n",
        "You'll also look at alternative optimization algorithms. These are of primary interest when one encounters local minimum. Knowing when one has hit such a pitfall can be challenging and typically requires experimenting with different optimization approaches and learning rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp9jBxBcg5Tw"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Over the next couple of weeks, you'll extend your neural network knowledge by learning about evaluation, normalization, and regularization. Learning these will help you select the optimal model. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (learn-env)",
      "language": "python",
      "name": "learn-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
